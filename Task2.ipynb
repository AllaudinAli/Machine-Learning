{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn6jYidqcQq2vP6CHx0mR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllaudinAli/Machine-Learning/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Part A**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K0-4XlS44vfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "s9Pjo2XAEGIM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib.image import imread\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from skimage.feature import hog\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding dataset"
      ],
      "metadata": {
        "id": "_p9gMkSwz9WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('BordeauxWines.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "gTi12moIEK-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d6c2cc-39c3-4e34-b8d8-a3ccbb40a887"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        Name  Year  Score Price  BLOOD ORANGE  \\\n",
            "0          ChÃ¢teau Croix Figeac St.-Emilion  2008     84  $20              0   \n",
            "1              ChÃ¢teau Fonroque St.-Emilion  2008     84  $29              0   \n",
            "2  ChÃ¢teau Grand Bertin de St.-Clair MÃ©doc  2008     84   $NA             0   \n",
            "3      ChÃ¢teau Lion Beaulieu Bordeaux White  2008     84   $NA             0   \n",
            "4           ChÃ¢teau Marsau CÃ´tes de Francs  2008     84  $20              0   \n",
            "\n",
            "   CITRUS  CITRUS PEEL  CITRUS ZEST  CLEMENTINE  LIME  ...  SKUNK  \\\n",
            "0       0            0            0           0     0  ...      0   \n",
            "1       0            0            0           0     0  ...      0   \n",
            "2       0            0            0           0     0  ...      0   \n",
            "3       0            0            0           0     0  ...      0   \n",
            "4       0            0            0           0     0  ...      0   \n",
            "\n",
            "   SULFUR DIOXIDE  WET WOOL,WET DOG  ACETIC ACID  ETHANOL  ETHYL ACETATE  \\\n",
            "0               0                 0            0        0              0   \n",
            "1               0                 0            0        0              0   \n",
            "2               0                 0            0        0              0   \n",
            "3               0                 0            0        0              0   \n",
            "4               0                 0            0        0              0   \n",
            "\n",
            "   ALCOHOL  FROTH  MENTHOL  SHERRY  \n",
            "0        0      0        0       0  \n",
            "1        0      0        0       0  \n",
            "2        0      0        0       0  \n",
            "3        0      0        0       0  \n",
            "4        0      0        0       0  \n",
            "\n",
            "[5 rows x 989 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitting two variables from the dataset\n",
        "Justifying:\n",
        "Given dataset has the large number of features around 980, it's reasonable to select a subset of features that are likely to be most informative for predicting wine quality.\n",
        "**I have Droped 'Name' and 'Price' features**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Because 'Name' does not effect the dataset what so ever and 'Price' feature is missing alot of data and have excessive outliers."
      ],
      "metadata": {
        "id": "aAZ_u4ug0L-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Name', 'Price'], axis=1)"
      ],
      "metadata": {
        "id": "luIjoFIvrez8"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing Score with Class Labels"
      ],
      "metadata": {
        "id": "4jpZ_ttv1tLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditions = [\n",
        "    (df['Score'] >= 90),\n",
        "    (df['Score'] >= 75) & (df['Score'] <= 89),\n",
        "    (df['Score'] <= 74)\n",
        "]\n",
        "labels = ['high', 'medium', 'low']\n",
        "df['Class'] = pd.Series(np.select(conditions, labels))\n",
        "\n",
        "df = df.drop('Score', axis=1)\n",
        "\n",
        "# Display the updated dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdMhy_0orlAt",
        "outputId": "624db123-e156-4f06-dc68-014846336b83"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year  BLOOD ORANGE  CITRUS  CITRUS PEEL  CITRUS ZEST  CLEMENTINE  LIME  \\\n",
            "0  2008             0       0            0            0           0     0   \n",
            "1  2008             0       0            0            0           0     0   \n",
            "2  2008             0       0            0            0           0     0   \n",
            "3  2008             0       0            0            0           0     0   \n",
            "4  2008             0       0            0            0           0     0   \n",
            "\n",
            "   GRAPEFRUIT  GRAPEFRUIT PEEL  ORANGE  ...  SULFUR DIOXIDE  WET WOOL,WET DOG  \\\n",
            "0           0                0       0  ...               0                 0   \n",
            "1           0                0       0  ...               0                 0   \n",
            "2           0                0       0  ...               0                 0   \n",
            "3           0                0       0  ...               0                 0   \n",
            "4           0                0       0  ...               0                 0   \n",
            "\n",
            "   ACETIC ACID  ETHANOL  ETHYL ACETATE  ALCOHOL  FROTH  MENTHOL  SHERRY  \\\n",
            "0            0        0              0        0      0        0       0   \n",
            "1            0        0              0        0      0        0       0   \n",
            "2            0        0              0        0      0        0       0   \n",
            "3            0        0              0        0      0        0       0   \n",
            "4            0        0              0        0      0        0       0   \n",
            "\n",
            "    Class  \n",
            "0  medium  \n",
            "1  medium  \n",
            "2  medium  \n",
            "3  medium  \n",
            "4  medium  \n",
            "\n",
            "[5 rows x 987 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting data into Train and Test"
      ],
      "metadata": {
        "id": "-12A5ipD18bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "m-5iHM9wwWPt"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Classification"
      ],
      "metadata": {
        "id": "a_uBwZ8J2DDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier = SVC()\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred = svm_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "nG9mORN4rmXk"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Accuracy"
      ],
      "metadata": {
        "id": "lCuShZ9H2wR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWkhHE5YpOMy",
        "outputId": "3609bbb4-c777-4b30-c16f-2018e69a1cf9"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.3588850174216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix and (FPR)"
      ],
      "metadata": {
        "id": "D0IWkhDl273n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calculate False Positive Rate (FPR) for each class\n",
        "num_classes = len(labels)\n",
        "fpr = {}\n",
        "for i in range(num_classes):\n",
        "    class_label = labels[i]\n",
        "    fp = sum(confusion_mat[:, i]) - confusion_mat[i, i]\n",
        "    tn = sum(sum(confusion_mat)) - sum(confusion_mat[i, :]) - sum(confusion_mat[:, i]) + confusion_mat[i, i]\n",
        "    fpr[class_label] = fp / (fp + tn)\n",
        "\n",
        "# Calculate overall accuracy of the model\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display False Positive Rate (FPR) for each class and overall accuracy\n",
        "for class_label, false_positive_rate in fpr.items():\n",
        "    print(f\"False Positive Rate ({class_label}): {false_positive_rate}\")\n",
        "print(\"Overall Accuracy:\", overall_accuracy*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f12cmPlhXODH",
        "outputId": "5af6f1fd-aa90-415f-b850-6ec397d7f96f"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[   0    0  819]\n",
            " [   0    0    3]\n",
            " [   0    0 2048]]\n",
            "False Positive Rate (high): 0.0\n",
            "False Positive Rate (medium): 0.0\n",
            "False Positive Rate (low): 1.0\n",
            "Overall Accuracy: 71.3588850174216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Part B**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jKCRuf9L4oVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating PCA Instance"
      ],
      "metadata": {
        "id": "GOfN5Nfj3mNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of PCA with the desired number of components\n",
        "pca = PCA(n_components=15)  # Example with 15 components\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "T2Uqhq9tptud"
      },
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating StandardScaler and Fitting Scaler on Training data"
      ],
      "metadata": {
        "id": "7utwLmS139r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform the data\n",
        "X_train_scaled = scaler.fit_transform(X_train_pca)\n",
        "X_test_scaled = scaler.transform(X_test_pca)"
      ],
      "metadata": {
        "id": "TfrTRGPVpwW0"
      },
      "execution_count": 397,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Classification"
      ],
      "metadata": {
        "id": "gN28LTv64N4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "y_pred = svm.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "pGO9NkIspzBP"
      },
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix / Accuracy / (FPR)"
      ],
      "metadata": {
        "id": "6kudZX_q4SGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
        "\n",
        "# Compute the False Positive Rate (FPR) for each class\n",
        "fpr_high = conf_matrix[0, 1:] / np.sum(conf_matrix[0, 1:])\n",
        "fpr_medium = conf_matrix[1, [0, 2]] / np.sum(conf_matrix[1, [0, 2]])\n",
        "fpr_low = conf_matrix[2, :-1] / np.sum(conf_matrix[2, :-1])\n"
      ],
      "metadata": {
        "id": "Ke3QmZDipy6F"
      },
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The Time taken for SVM Classification using raw data is around 2mins, while the time taken with PCA is around 6sec. This is because\n",
        "\n",
        "*   Increasing the number of components in PCA can improve accuracy by retaining more information from the original data.\n",
        "*   PCA simplifies the data by projecting it onto a lower-dimensional subspace, making the classification process more efficient. \n",
        "\n",
        "Therefore, PCA offers faster classification times and potentially improved accuracy by capturing the most important features."
      ],
      "metadata": {
        "id": "XHMF3gUm43vV"
      }
    }
  ]
}